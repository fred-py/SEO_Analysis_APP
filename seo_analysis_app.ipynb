{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The app will check the following:\\nWarnings, Title, Meta Description, Headings, Image Alt, Keywords'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The app will check the following:\n",
    "Warnings, Title, Meta Description, Headings, Image Alt, Keywords\"\"\"\n",
    "\n",
    "# resource video PART1 https://www.youtube.com/watch?v=1Y-x59e90Nw\n",
    "# resource video PART2 https://youtu.be/j7TLgyTrtp8\n",
    "\n",
    "# resource static https://pythonology.eu/build-an-seo-analyzer-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/frederico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/frederico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from keys import api_keys\n",
    "# Own Functions <----\n",
    "from var import res, soup, title, meta_d\n",
    "from check_length_module import check_length # Fred's function\n",
    "\n",
    "\n",
    "# nltk for natural language processing\n",
    "# To look atr frequency of keywords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize # GEt a single token eg. single word from the text\n",
    "nltk.download(\"stopwords\") # Words that aren't helpful when analysing keywords eg. the\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: 'Home - United Property Services' has 31 characters\n",
      "The ideal length is 50-60 characters.\n",
      "\n",
      "META DESCRIPTION: 'New name, same great service! We’ve rebranded, and Elite is now United, which marks the next chapter in our journey.' has 116 characters\n",
      "The ideal length is 150-160 characters.\n",
      "\n",
      "All images contain 'Alt Attribute'.\n",
      "\n",
      "Keywords FreqDist: [('cleaning', 10), ('services', 8), ('damage', 7), ('concrete', 6), ('carpet', 6), ('united', 5), ('dirt', 5), ('clean', 5), ('water', 4), ('restoration', 4)]\n",
      "OK: ['Title exists: Home - United Property Services', 'Meta Description exists: New name, same great service!\\xa0We’ve rebranded, and Elite is now United,\\xa0which marks the next chapter in our journey.', 'h1-->UNITED PROPERTY SERVICES', 'h2-->Comprehensive Cleaning & Restoration Services', 'h3-->Welcome to United', 'h2-->Our Services', 'h2-->CARPET & RUG DRY CLEANING', 'h2-->LOUNGES & UPHOLSTERY', 'h2-->TILE & GROUT CLEANING', 'h2-->CONCRETE CLEANING', 'h2-->CURTAINS', 'h2-->LEATHER CLEAN & PROTECTION', 'h2-->WATER DAMAGE & RESTORATION\\u200b', 'h3-->beyond the surface. Carpet & upholstery hygiene.', 'h3-->SERVICING THE SOUTH WEST', 'h5-->Get in Touch']\n",
      "WARNING: []\n"
     ]
    }
   ],
   "source": [
    "# Webpage to analyse\n",
    "# For Streamlit url = input(f\"Enter url to run analysis: \")\n",
    "url = \"https://unitedpropertyservices.au/\"\n",
    "\n",
    "\n",
    "def analyse_url(url):\n",
    "    \"\"\"Analyse headings, keywords & alt attribute in images\"\"\"\n",
    "    warning = [] # Warning may including missing tile, meta-content, alt etc...\n",
    "    ok = []      # Includes good titles, headings, descriptions etc...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if title:  # Add title to ok list if title exists\n",
    "        ok.append(f\"Title exists: {title}\")\n",
    "        check_length(title)\n",
    "    else:      # If no title then add to warning list  \n",
    "        warning.append(f\"Title is missing!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if meta_d: # Add meta description to ok list if title exists\n",
    "        ok.append(f\"Meta Description exists: {meta_d}\")\n",
    "        check_length(meta_d)\n",
    "    else:\n",
    "        warning.append(f\"Meta Description is missing\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hs = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"] # Grab Headings\n",
    "    h_tags = []\n",
    "\n",
    "\n",
    "    for h in soup.find_all(hs): \n",
    "        ok.append(f\"{h.name}-->{h.text.strip()}\")\n",
    "        h_tags.append(h.name)\n",
    "    if \"h1\" not in h_tags:\n",
    "        warning.append(\"No H1 found!\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i in soup.find_all(\"img\", alt=\" \"): # Extract the images without Alt\n",
    "        warning.append(f\"No Alt: {i}\")\n",
    "    else:\n",
    "        print(\"All images contain 'Alt Attribute'.\\n\")\n",
    "\n",
    "    # Extract keywords\n",
    "    bod = soup.find(\"body\").text # Grab text from body of the HTML\n",
    "    # Add words inside list \n",
    "    words = [i.lower() for i in word_tokenize(bod)] # Return i for i in word-tokenize\n",
    "    \n",
    "    # Grab a list of English stopwords (actual words not from url)\n",
    "    sw = nltk.corpus.stopwords.words(\"english\")\n",
    "    keywords = []\n",
    "\n",
    "    for i in words:                     # If found in words variable, stop words are to be excluded.\n",
    "        if i not in sw and i.isalpha(): # isalpha stands for actual words and not symbols etc...\n",
    "            keywords.append(i)\n",
    "    \n",
    "     \n",
    "\n",
    "    \n",
    "    freq = nltk.FreqDist(keywords) # Check frequency distribution of keywords\n",
    "\n",
    "    print(f\"Keywords FreqDist: {freq.most_common(10)}\") # Check most common top 10\n",
    "    print(f\"OK: {ok}\")\n",
    "    print(f\"WARNING: {warning}\")\n",
    "\n",
    "\n",
    "\n",
    "analyse_url(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recommendations Dict (may not be needed but it is here for now)\n",
    "rec = {\n",
    "    \"title\" : \" \",\n",
    "    \"meta description\" : \" \",\n",
    "    \"headings\" : \" \",\n",
    "    \"img alt\" : \" \",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url).text \n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bod var looks for body tag on html and grabs text. This skips all JS and CSS in between\n",
    "bod = soup.find(\"body\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['home',\n",
       " 'concrete',\n",
       " 'cleaning',\n",
       " 'water',\n",
       " 'damage',\n",
       " '&',\n",
       " 'restoration',\n",
       " 'carpet',\n",
       " '&',\n",
       " 'rug']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add words inside list\n",
    "words = [i.lower() for i in word_tokenize(bod)] # Return i for i in word-tokenize\n",
    "words[:10] # Run top 10 or more and see what stop words come up then get rid of then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cleaning', 10), ('services', 8), ('damage', 7), ('concrete', 6), ('carpet', 6), ('united', 5), ('dirt', 5), ('clean', 5), ('water', 4), ('restoration', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is an actual list of all English stop words\n",
    "sw = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# If found in words variable, stop words are to be excluded.\n",
    "new_words = []\n",
    "for i in words:\n",
    "    if i not in sw and i.isalpha(): # isalpha stands for actual words and not symbols etc...\n",
    "        new_words.append(i)\n",
    "\n",
    "# Check frequency distribution of keywords\n",
    "freq = nltk.FreqDist(new_words)\n",
    "# Check most common top 10\n",
    "print(freq.most_common(10))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seo_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
